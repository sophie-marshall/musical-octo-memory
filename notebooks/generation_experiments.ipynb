{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation Experiments \n",
    "\n",
    "The final step in this process is using results from our retrieval step to inform the response we get back from an LLM! \n",
    "\n",
    "Limiting the models knowledge to the context provided by us helps reduce probability of hallucinations and improve the accuracy of our responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import subprocess\n",
    "\n",
    "# get root of current repo and add to our path\n",
    "root_dir = subprocess.check_output([\"git\", \"rev-parse\", \"--show-toplevel\"], stderr=subprocess.DEVNULL).decode(\"utf-8\").strip()\n",
    "\n",
    "sys.path.append(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval + Prompt Construction \n",
    "\n",
    "Grab results from our database and build a prompt with context to feed to an LLM of our choice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.postgres import PostgresClient\n",
    "\n",
    "results = PostgresClient.hybrid_search(\"Tell me about your education\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\n",
    "for result in results:\n",
    "    context += result[\"clean_text\"] + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(query: str, context: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "        You are a helpful assistant for Sophie Marshall. A Data Engineer seeking new opprotunities in the Data Engineering field.\n",
    "        Your task is to answer the user's query based on the provided context. If the context does not contain enough information \n",
    "        to answer the query, respond with \"I don't know. Would you like to follow up with Sophie directly?\".\n",
    "\n",
    "        <query>{query}</query>\n",
    "        <context>{context}</context>\n",
    "    \"\"\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = generate_prompt(\"Tell me about your education\", context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.constants import TEST_QUERIES\n",
    "\n",
    "query_prompt_dict = {}\n",
    "\n",
    "for query in TEST_QUERIES:\n",
    "    search_results = PostgresClient.hybrid_search(query)\n",
    "    context = \"\"\n",
    "    for result in search_results:\n",
    "        context += result[\"clean_text\"] + \"\\n\"\n",
    "    prompt = generate_prompt(query, context)\n",
    "    query_prompt_dict[query] = prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation\n",
    "\n",
    "Until this is ready to go live, I'll use ChatGPT to get my point across. I want to avoid paying for API calls until I need to. \n",
    "\n",
    "Here's how it works: \n",
    "- I'll run the hybrid search here in the notebook \n",
    "- The `generate_prompt` function will return a string of text that I will feed to ChatGPT that contains the returned context from hybrid search as well as some response instructions \n",
    "- I will post the response from ChatGPT into this notebook to see what our agent might say in the future!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Query:` \n",
    "- Tell me about yourself\n",
    "\n",
    "`Response:` \n",
    "- Sophie Marshall is a data engineer with a strong interdisciplinary background in psychology, economics, and mathematical modeling. She graduated from the University of Wisconsin–Madison in 2022 with a Bachelor of Science in Psychology and Economics with a mathematical emphasis. During her undergraduate years, she was a Division I athlete on the Wisconsin women’s rowing team, where she developed skills in discipline, teamwork, and resilience. Her work combines expertise in data engineering, machine learning, and analytics. One of her key accomplishments is the design and deployment of a retrieval-augmented generation (RAG) powered chatbot that enables natural language search across internal datasets. Sophie is currently seeking mid-level data or analytics engineering roles in the DC metro area, with a preference for hybrid work but openness to remote or in-person opportunities where the fit is right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Query:`\n",
    "- What is your educational background\n",
    "\n",
    "`Response:`\n",
    "- Sophie Marshall graduated from the University of Wisconsin–Madison in 2022 with a Bachelor of Science in Psychology and Economics, with a mathematical emphasis. Her academic background reflects a strong interdisciplinary foundation in human cognition, data modeling, statistical analysis, and systems thinking—skills that directly support her work in data engineering and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Query:`\n",
    "- Why are you seeking a new position?\n",
    "\n",
    "`Response:`\n",
    "- Sophie is seeking a new position because the grant supporting her work on PBS’s Innovation Team concludes in June 2025. She’s looking for her next opportunity—ideally one where she can continue building intelligent data systems that bridge infrastructure and user experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Query:`\n",
    "- What experience do you have with data pipelines?\n",
    "\n",
    "`Response:`\n",
    "- Sophie has extensive experience designing and building data pipelines that integrate machine learning, data engineering, and backend infrastructure. At PBS’s Digital Innovation Team, she has led projects involving dynamic pipeline development, ETL workflows, vector database integration, and retrieval-augmented systems. Her work includes building robust data workflows using Python, Apache Airflow, and AWS Step Functions, as well as designing data labeling pipelines for machine learning model training and fine-tuning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resume-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
