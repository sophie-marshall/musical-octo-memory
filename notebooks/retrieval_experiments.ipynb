{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Experiments\n",
    "\n",
    "Relevance of context returned from search greatly impacts the quality of our RAG system. This notebook will explore what retrieval methods are available to us with the goal of producing the most relevant content to a users query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import subprocess\n",
    "\n",
    "# get root of current repo and add to our path\n",
    "root_dir = subprocess.check_output([\"git\", \"rev-parse\", \"--show-toplevel\"], stderr=subprocess.DEVNULL).decode(\"utf-8\").strip()\n",
    "\n",
    "sys.path.append(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup \n",
    "\n",
    "Before we get started, let's initialize our clients, models, and test params for re-use throguhout this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for consistency, define a set of test queries \n",
    "test_queries = [\n",
    "    \"tell me about yourself\", \n",
    "    \"what is your educational background\", \n",
    "    \"why are you seeking a new position\", \n",
    "    \"what experience do you have with data pipelines\", \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srmarshall/.virtualenvs/rag/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# instantiate the model\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.postgres import PostgresClient\n",
    "import os \n",
    "import psycopg2\n",
    "from pgvector.psycopg2 import register_vector\n",
    "\n",
    "\n",
    "class PostgresClient:\n",
    "    def __init__(self, pg_host: str, pg_user: str, pg_password: str, pg_db: str):\n",
    "        self.pg_host = pg_host\n",
    "        self.pg_user = pg_user\n",
    "        self.pg_password = pg_password\n",
    "        self.pg_db = pg_db\n",
    "\n",
    "    def _make_conn(self):\n",
    "        try:\n",
    "            conn = psycopg2.connect(\n",
    "                host=self.pg_host,\n",
    "                user=self.pg_user,\n",
    "                password=self.pg_password,\n",
    "                dbname=self.pg_db,\n",
    "            )\n",
    "            register_vector(conn)\n",
    "            return conn\n",
    "        except Exception as e:\n",
    "            print(f\"Error connecting to Postgres: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def insert_content_embeddings(self, data: list):\n",
    "        try:\n",
    "            conn = self._make_conn()\n",
    "            cursor = conn.cursor()\n",
    "            for record in data:\n",
    "                try:\n",
    "                    cursor.execute(\n",
    "                        \"\"\"\n",
    "                                INSERT INTO content_embeddings(document_id, tags, clean_text, embedding)\n",
    "                                    VALUES(%s, %s, %s, %s);\n",
    "                            \"\"\",\n",
    "                        (\n",
    "                            record[\"document_id\"],\n",
    "                            record[\"tags\"],\n",
    "                            record[\"clean_text\"],\n",
    "                            record[\"embedding\"],\n",
    "                        ),\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Error inserting record: {str(e)}\")\n",
    "            conn.commit()\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "        except Exception as e:\n",
    "            print(f\"Error connecting to Postgres: {str(e)}\")\n",
    "\n",
    "    def semantic_search(self, query_embedding: list, n_results: int = 5):\n",
    "        # convert query embedidng to the proper format\n",
    "        query_embedding_str = \", \".join(map(str, query_embedding))\n",
    "\n",
    "        # build query\n",
    "        search = f\"SELECT document_id, tags, clean_text, 1 - (embedding <=> '[{query_embedding_str}]') as similarity_score FROM content_embeddings ORDER BY embedding <=> '[{query_embedding_str}]' LIMIT {n_results};\"\n",
    "\n",
    "        # establish a connection to the DB\n",
    "        conn = self._make_conn()\n",
    "        try:\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(search)\n",
    "            results = cursor.fetchall()\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"Error conduting semantic search: {str(e)}\")\n",
    "            return None\n",
    "        finally:\n",
    "            cursor.close()\n",
    "\n",
    "    def hybrid_search(self, query_text: str, query_embedding: list, n_results: int = 5, similarity_threshold: float = 0.3):\n",
    "    keywords = [word.lower() for word in query_text.split()]\n",
    "    query_embedding_str = \", \".join(map(str, query_embedding))\n",
    "\n",
    "    conn = self._make_conn()\n",
    "    if not conn:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        for keyword in keywords:\n",
    "            cursor.execute(f\"\"\"\n",
    "                SELECT * FROM (\n",
    "                    SELECT \n",
    "                        document_id,\n",
    "                        tags,\n",
    "                        clean_text,\n",
    "                        1 - (embedding <=> '[{query_embedding_str}]') AS similarity_score,\n",
    "                        (\n",
    "                            SELECT MAX(similarity(tag, %s::text))\n",
    "                            FROM unnest(tags) AS tag\n",
    "                            WHERE similarity(tag, %s::text) > %s\n",
    "                        ) AS tag_score\n",
    "                    FROM content_embeddings\n",
    "                ) AS ranked_results\n",
    "                ORDER BY similarity_score + COALESCE(tag_score, 0) DESC\n",
    "                LIMIT %s;\n",
    "            \"\"\", (keyword, keyword, similarity_threshold, n_results))\n",
    "\n",
    "            rows = cursor.fetchall()\n",
    "            if rows:\n",
    "                return rows\n",
    "\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error conducting hybrid search: {str(e)}\")\n",
    "        return None\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# initialize a postgres client \n",
    "pg = PostgresClient(\n",
    "    pg_host=os.getenv(\"PG_HOST\"),\n",
    "    pg_user=os.getenv(\"PG_USER\"),\n",
    "    pg_password=os.getenv(\"PG_PASSWORD\"),\n",
    "    pg_db=\"resume_rag\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity Search \n",
    "\n",
    "Let's start simple with `pgvector` cosine similarity search. This approach will take a query embedding and calculate the cosine similarity between each observation in our database. \n",
    "\n",
    "\n",
    "**Strengths:**\n",
    "- Excellent response to why searching and tell me about yourself. There are documents in the knowledge base speaking directly to these topics and this metric does a good job surfacing them \n",
    "\n",
    "**Weaknesses:**\n",
    "- Results for educational background are not what we want. As expected, there is some confusion between \"Education Analytics\" and formal education at university\n",
    "- Results for experience with pipelines misses the mark a bit. The first result is great, but the subsequent two aren't really related to the question. \n",
    "\n",
    "**Next Steps:**\n",
    "- Find a way to differentiate Education Analytics from \"education\" when referring to college or unitversity\n",
    "- Add additional documents that speak to specific experience with technologies or projects. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Query:  tell me about yourself\n",
      "Top 3 Results:\n",
      "- skills that directly support her current work in data engineering and machine learning while studying psychology sophie developed a deep interest in the mechanisms of human cognition which naturally led her to explore fields like artificial intelligence and machine learning her coursework in\n",
      "- sophie marshall graduated from the university of wisconsin madison in 2022 with a bachelor of science in psychology and economics with a mathematical emphasis her academic training reflects a strong interdisciplinary foundation in human cognition data modeling and statistical analysis skills that\n",
      "- her coursework in economics paired with a focus on mathematical modeling helped her build a solid foundation in systems thinking multivariate analysis and data driven decision making during her undergraduate years sophie was a division i athlete competing as a four year member of the wisconsin\n",
      "\n",
      "\n",
      "Test Query:  what is your educational background\n",
      "Top 3 Results:\n",
      "- education analytics conducts research and develops rigorous analytics that support actionable solutions and drive continuous improvement in american education we help our partners make better decisions on policies and programs that lead to success for all students sophie spent 1 5 years at education\n",
      "- years at education analytics where she held roles as both a data analyst intern and later as an assistant research analyst over this time she contributed to projects at the intersection of educational research data engineering and stakeholder facing analytics as a data analyst intern sophie focused\n",
      "- skills that directly support her current work in data engineering and machine learning while studying psychology sophie developed a deep interest in the mechanisms of human cognition which naturally led her to explore fields like artificial intelligence and machine learning her coursework in\n",
      "\n",
      "\n",
      "Test Query:  why are you seeking a new position\n",
      "Top 3 Results:\n",
      "- sophie is searching for mid level data or analytics engineering positions in the dc metro area while she prefers a hybrid work style she is open to full time remote or in person roles if the fit is right\n",
      "- as the innovation team s grant concludes in june 2025 sophie is actively seeking her next opportunity ideally one where she can continue building intelligent data systems that bridge infrastructure and user experience\n",
      "- sophie has spent the past two years as a data engineer on pbs s innovation team a grant funded r d group tasked with exploring emerging technologies and identifying ways they can serve public media stations and audiences in this role she has led the development of backend infrastructure data\n",
      "\n",
      "\n",
      "Test Query:  what experience do you have with data pipelines\n",
      "Top 3 Results:\n",
      "- infrastructure data pipelines and ml integrated systems that bring together siloed data sources across pbs her work includes building robust data workflows using python apache airflow and aws step functions with projects spanning etl pipelines vector database integration and retrieval augmented\n",
      "- sophie is a data engineer passionate about building intelligent resilient pipelines that help teams keep pace with today s rapidly evolving data landscape she brings a strong foundation in ml integrated systems backend development and full stack prototyping currently sophie serves as the data\n",
      "- data engineering infrastructure\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query in test_queries:\n",
    "    print(\"Test Query: \", query)\n",
    "\n",
    "    query_embedding = model.encode(query)\n",
    "    results = pg.semantic_search(query_embedding, n_results=3)\n",
    "    print(\"Top 3 Results:\")\n",
    "    for result in results:\n",
    "        print(f\"- {result[2]}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag Augementation \n",
    "\n",
    "We want some way to bolster responses if a word from the query either exactly matches or closely matches a word from a tag. The hope is that this addresses the confusion between \"Education Analytics\" and education as it relates to college. \n",
    "\n",
    "An additional layer of security we can add later down the line is to explicitly provide this information in the system prompt. However, this doesnt address the problem of irrelevant information taking the place of relevant information in the 3 returned DB results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error conducting hybrid search: column \"similarity_score\" does not exist\n",
      "LINE 13:                     ORDER BY similarity_score + COALESCE(tag...\n",
      "                                      ^\n",
      "\n"
     ]
    }
   ],
   "source": [
    "education_query = test_queries[1]\n",
    "\n",
    "# cosine search with a bigger set of results \n",
    "hybrid_search_results = pg.hybrid_search(education_query, query_embedding, n_results=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what', 'is', 'your', 'educational', 'background']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "education_query.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['job', 'professional', 'experience', 'work history']\n",
      "['summary', 'professional summary', 'elevator pitch']\n",
      "['job', 'professional', 'experience', 'work history']\n",
      "['job', 'professional', 'experience', 'work history']\n",
      "['summary', 'professional summary', 'elevator pitch']\n",
      "['job', 'professional', 'experience', 'work history']\n",
      "['education', 'university', 'college', 'degree']\n",
      "['job', 'professional', 'experience', 'work history']\n",
      "['job', 'professional', 'experience', 'work history']\n",
      "['job', 'professional', 'experience', 'work history']\n",
      "['job', 'professional', 'experience', 'work history']\n",
      "['summary', 'professional summary', 'elevator pitch']\n",
      "['job', 'professional', 'experience', 'work history']\n",
      "['job', 'professional', 'experience', 'work history', 'internship']\n",
      "['job', 'professional', 'experience', 'work history', 'internship']\n",
      "['looking for', 'job_search', 'professional']\n",
      "['education', 'university', 'college', 'degree']\n",
      "['job', 'professional', 'experience', 'work history', 'internship']\n",
      "['job', 'professional', 'experience', 'work history']\n",
      "['education', 'university', 'college', 'degree']\n",
      "['job', 'professional', 'experience', 'work history', 'internship']\n",
      "['job', 'professional', 'experience', 'work history', 'internship']\n",
      "['job', 'professional', 'experience', 'work history']\n",
      "['job', 'professional', 'experience', 'work history', 'internship']\n",
      "['job', 'professional', 'experience', 'work history']\n",
      "['education', 'university', 'college', 'degree']\n",
      "['job', 'professional', 'experience', 'work history']\n",
      "['education', 'university', 'college', 'degree']\n",
      "['personal', 'interests', 'hobbies', 'outside work']\n",
      "['personal', 'interests', 'hobbies', 'outside work']\n"
     ]
    }
   ],
   "source": [
    "for result in cosine_similarity_results:\n",
    "    print(result[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
