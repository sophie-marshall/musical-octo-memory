{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `ResumeRAG` Data Pipeline Testing \n",
    "\n",
    "Let's explore how we might:\n",
    "- Gather data to serve as our knowledge base \n",
    "- Transform and enrich the data to meet our use cases specific needs \n",
    "- Load the data into a vector store of choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import subprocess\n",
    "\n",
    "# get root of current repo and add to our path\n",
    "root_dir = subprocess.check_output([\"git\", \"rev-parse\", \"--show-toplevel\"], stderr=subprocess.DEVNULL).decode(\"utf-8\").strip()\n",
    "\n",
    "sys.path.append(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction \n",
    "\n",
    "The purpose of our `ResumeRAG` system is to allow others to ask the system questions about your professional history. To do this succesfully, the system needs to be well hydrated with accurate and detailed information about said history. \n",
    "\n",
    "Step 1 is to create some documents detailing the information you'd like to be available to your users and read it into your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(f\"{root_dir}/data\")\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for file_name in data_dir.iterdir():\n",
    "    with open(file_name, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    data_dict[file_name.name] = {\n",
    "        \"raw_content\": content\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation \n",
    "\n",
    "Before we're ready to embed, we want to: \n",
    "- Standardize the text data\n",
    "- Enrich it with content tags to help our search results later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.helpers import strip_text\n",
    "\n",
    "# clean up raw content a bit\n",
    "for file_name, content in data_dict.items():\n",
    "    clean_content = strip_text(content[\"raw_content\"])\n",
    "    data_dict[file_name][\"clean_content\"] = clean_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagging \n",
    "\n",
    "This will be pretty manual, but next I'll add tags to each piece of content. The goal of these tags are to help ensure our retrieval mechanism returns relevant information. \n",
    "\n",
    "I decided to add tags that add context to the ___ of the text within the document. My hope is that this will help ensure when people ask about \"work\" they only get \"work\" or \"professional\" tagged content. Or ensure that if someone asks about my education, we can mitigate confusion that might arise from an employed with \"Education\" in the title and my actual University education.\n",
    "\n",
    "If we see results are better/worse than we expect, we can always modify these tags as one method of imprvment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict[\"looking_for.txt\"][\"tags\"] = [\"looking for\", \"job_search\", \"professional\"]\n",
    "data_dict[\"education.txt\"][\"tags\"] = [\"education\", \"university\", \"college\", \"degree\"]\n",
    "data_dict[\"summary.txt\"][\"tags\"] = [\"summary\", \"professional summary\", \"elevator pitch\"]\n",
    "data_dict[\"personal.txt\"][\"tags\"] = [\"personal\", \"interests\", \"hobbies\", \"outside work\"]\n",
    "data_dict[\"pbs.txt\"][\"tags\"] = [\"job\", \"professional\", \"experience\", \"work history\"]\n",
    "data_dict[\"education_analytics.txt\"][\"tags\"] = [\"job\", \"professional\", \"experience\", \"work history\", \"internship\"]\n",
    "data_dict[\"hive.txt\"][\"tags\"] = [\"job\", \"professional\", \"experience\", \"work history\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking\n",
    "\n",
    "In RAG systems, chunking text helps improve the retrieval accuracy of the system. To do this we'll use an available Langchain tool to split our text into chunk sized of 300. Additionally, well add some overlap to ensure content continuity and minimize loss of context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# convert to DF for ease \n",
    "df = pd.DataFrame.from_dict(data_dict, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# instantiate text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=20)\n",
    "\n",
    "# add new split texts row \n",
    "df[\"split_text\"] = df[\"clean_content\"].apply(text_splitter.split_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast long so each row is a single split text\n",
    "df_long = df.explode('split_texts').reset_index()\n",
    "\n",
    "# quick rename\n",
    "df_long = df_long.rename(columns={'index': 'document_id'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "\n",
    "Finally, we'll embed our cleaned and chunked text! This is where the magic of RAG really lies. By embedding the text, we make it machine interpretable. This will help us bridge the gap between human language and computer understanding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srmarshall/.virtualenvs/rag/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# instantiate the model\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed each chunk \n",
    "df_long[\"embedding\"] = df_long[\"split_texts\"].apply(lambda x: model.encode(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading \n",
    "\n",
    "The final step is to get this content into our selected vector store! For this project I picked PostgreSQL. If you want to learn more about how the database was set up, check out the `database_setup` notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset to the columns of interest\n",
    "subset = df_long[[\"document_id\", \"tags\", \"split_texts\", \"embedding\"]]\n",
    "\n",
    "subset = subset.rename(columns={'split_texts': 'clean_text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to list for client\n",
    "data = subset.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.postgres import PostgresClient\n",
    "import os \n",
    "\n",
    "pg = PostgresClient(\n",
    "    pg_host=os.getenv(\"PG_HOST\"),\n",
    "    pg_user=os.getenv(\"PG_USER\"),\n",
    "    pg_password=os.getenv(\"PG_PASSWORD\"),\n",
    "    pg_db=\"resume_rag\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg.insert_content_embeddings(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
